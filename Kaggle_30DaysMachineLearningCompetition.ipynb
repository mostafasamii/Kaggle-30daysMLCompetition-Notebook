{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#Importing the models\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder,OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold\n# For training random forest model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.decomposition import PCA\nfrom category_encoders.m_estimate import MEstimateEncoder\nfrom lightgbm import LGBMRegressor\n#from tabgan.sampler import OriginalGenerator, GANGenerator\nfrom sklearn.ensemble import VotingRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nimport os\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.decomposition import PCA\nimport warnings\nfrom tqdm import tqdm\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-30T19:02:53.392959Z","iopub.execute_input":"2021-08-30T19:02:53.393745Z","iopub.status.idle":"2021-08-30T19:02:56.691929Z","shell.execute_reply.started":"2021-08-30T19:02:53.393642Z","shell.execute_reply":"2021-08-30T19:02:56.691139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"# Reading data\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n#Load train and test data\ntrain = pd.read_csv(\"../input/30-days-of-ml/train.csv\", index_col=0)\ntest = pd.read_csv(\"../input/30-days-of-ml/test.csv\", index_col=0)\nsub = pd.read_csv('../input/submissionnn/submission.csv')\n\n#Remove the outliers\nmean = train['target'].mean()\nstd = train['target'].std()\ncut_off = std * 3\nlower, upper = mean - cut_off, mean + cut_off\noutliers = train[(train['target'] < lower) | (train['target'] > upper)]\nprint(f\"Orginal Dataset size: {train.shape}\")\ntrain.drop(outliers.index.to_list(), inplace=True)\nprint(f\"Number of outliers: {len(outliers)}\")\n\n#Separate target from features\ntarget = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n#List of numerical columns\nnum_cols = [col for col in features.columns if 'cont' in col]\n\n# Make copy to avoid changing original data \nlabel_features = features.copy()\nlabel_test = test.copy()\n\n# Label encoding for categorical features\nlabel = LabelEncoder()\nlabel_features[object_cols] = label_features[object_cols].apply(label.fit_transform)\nlabel_test[object_cols] = label_test[object_cols].apply(label.fit_transform)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T19:02:56.693471Z","iopub.execute_input":"2021-08-30T19:02:56.694035Z","iopub.status.idle":"2021-08-30T19:03:05.840789Z","shell.execute_reply.started":"2021-08-30T19:02:56.694001Z","shell.execute_reply":"2021-08-30T19:03:05.839759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up fold parameters\nsplits = 10                        #5   >>>> 5 increased the MSE\nkf = KFold(n_splits=splits, shuffle=True, random_state=0)\n#Generating folds and making training and prediction for each of 10 folds\nfor train_index, test_index in kf.split(label_features, target):\n    X_train, X_valid = label_features.iloc[list(train_index)], label_features.iloc[list(test_index)]\n    y_train, y_valid = target.iloc[list(train_index)], target.iloc[list(test_index)]\n#-----------------------------------------------------------------------------------------\nX_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"xgb_params = {\n    'lambda': 67.79737006663706,\n    'alpha': 40.12405005448161,\n    'colsample_bytree': 0.061613774851329205,\n    'subsample': 0.9556736521337416,\n    'learning_rate': 0.17024722721525629,\n    'n_estimators': 10000,\n    'max_depth': 3,\n    'booster': 'gbtree',\n    'min_child_weight': 155,\n    'booster': 'gbtree',\n    'seed' : 38,\n    'random_state':42   #removable doesn't exist from the begining \n\n}\nearly_sr = 300   #100\nXGB_model = XGBRegressor(n_jobs=4,**xgb_params)\nXGB_model.fit(X_train, y_train, verbose=1000,\n             early_stopping_rounds=early_sr, \n             eval_set=[(X_valid, y_valid)],\n             eval_metric= 'rmse')\n# Predictions on testing data\npredictions_XGBoost_1 = XGB_model.predict(label_test)\n\n\n#First model parameters trial  >>> chosen\nparams = {\n    'learning_rate': 0.07853392035787837,\n    'reg_lambda': 1.7549293092194938e-05,\n    'reg_alpha': 14.68267919457715, \n    'subsample': 0.8031450486786944, \n    'colsample_bytree': 0.170759104940733, \n    'max_depth': 3,\n    'n_estimators': 5000\n    #'random_state':40\n}\nearly_sr = 300   #100\nXGB_model = XGBRegressor(n_jobs=4,**params)\nXGB_model.fit(X_train, y_train, verbose=1000,\n             early_stopping_rounds=early_sr, \n             eval_set=[(X_valid, y_valid)],\n             eval_metric= 'rmse')\n# Predictions on testing data\npredictions_XGBoost_2 = XGB_model.predict(label_test)\n\nxgb_params = {\n    'n_estimators': 10000,\n    'learning_rate': 0.03628302216953097,\n    'subsample': 0.7875490025178415,\n    'colsample_bytree': 0.11807135201147481,\n    'max_depth': 3,\n    'booster': 'gbtree', \n    'reg_lambda': 0.0008746338866473539,\n    'reg_alpha': 23.13181079976304,\n    'random_state':42   #40 origin of random state\n}\nearly_sr = 300   #100\nXGB_model = XGBRegressor(n_jobs=4,**xgb_params)\nXGB_model.fit(X_train, y_train, verbose=1000,\n             early_stopping_rounds=early_sr, \n             eval_set=[(X_valid, y_valid)],\n             eval_metric= 'rmse')\n# Predictions on testing data\npredictions_XGBoost_3 = XGB_model.predict(label_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensembling","metadata":{}},{"cell_type":"code","source":"ss = sub['target'].tolist()\nfinal_predictions = [(ele1+ele2+ele3+ele4)/4 for ele1, ele2, ele3, ele4 in zip(ss, predictions_XGBoost_1, predictions_XGBoost_2, predictions_XGBoost_3)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submitting","metadata":{}},{"cell_type":"code","source":"# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': label_test.index,\n                       'target': final_predictions})\noutput.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}