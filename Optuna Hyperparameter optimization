def objective(trial, data=label_features, target=target):

    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2, random_state=0)
    param = {
        'metric': 'rmse', 
        'n_estimators': 20000,
        'learning_rate': trial.suggest_loguniform("learning_rate", 1e-2, 0.25),
        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-2, 100.),
        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.01, 1.0),
        'subsample': trial.suggest_categorical('subsample', [0.2,0.3,0.4,0.5,0.6,0.7,0.8,1.0]),
        'subsample_freq': trial.suggest_int('subsample_freq', 1, 20),
        'max_depth': trial.suggest_categorical('max_depth', [-1,30,100,300]),
        'num_leaves' : trial.suggest_int('num_leaves', 2, 500),
        'min_child_samples': trial.suggest_int('min_child_samples', 1, 200),
        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-3, 10),
        'cat_smooth' : trial.suggest_int('cat_smooth', 1, 100)
    }

    model = LGBMRegressor(**param)  
    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=300,verbose=False)

    preds = model.predict(test_x)    
    rmse = mean_squared_error(test_y, preds,squared=False)

    return rmse

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)
print('Best trial:', study.best_params)
